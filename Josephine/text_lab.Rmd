---
title: "text_lab"
author: "Brian Wright"
date: "9/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Congratulations you've successfully transferred from being a NBA 'quant' scout to a consultant specializing in US national sentiment! You've been hired by a non-profit in secret to track the level of support nationally and regionally for the Climate Change issues. The goal is to get a general idea of patterns associated with articles being written on the broad topic of Climate Change (you can also choose to select a sub-topic). In doing so your data science team has decided to explore periodicals from around the country in a effort to track the relative positive or negative sentiment and word frequencies. Luckily you have access to a world class library search engine call LexusNexus (NexusUni) that provides access to newspapers from around the country dating back decades. You'll first need to decided what words you want to track and what time might be interesting to begin your search. 

Your main goal (and the goal of all practicing data scientists!) is to translate this information into action. What patterns do you see, why do you believe this to be the case? What additional information might you want? Be as specific as possible, but keep in mind this is an initial exploratory effort...more analysis might be needed...but the result can and should advise the next steps you present to the firm. 

## MidAtlantic Region 

#### Loading Essential Libraries 
```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
library(ggwordcloud) 
library(textdata)
library(rtf)
```

#### Load in Mid Atlantic Region File
```{r} 
# import data from compiled newspaper reading
mid_atl <- read_lines("MidAtlanticRegion.txt")
# make mid_atl into a tibble data structure
mid_atl <- tibble(mid_atl)
```

#### Word Frequencies
1. The first step in the process is finding the frequency of words per region. 
```{r}
#change the data frame variables in mid_atl$mid_atl character types
mid_atl$mid_atl <- as.character(mid_atl$mid_atl)

mid_atl <- mid_atl %>% 
  unnest_tokens(word, mid_atl) %>%
  anti_join(stop_words) %>% count(word, sort=TRUE)
```

#### Sentiment Analysis 
```{r}
mid_atl_sentiment_affin <- mid_atl %>% inner_join(get_sentiments('afinn'))
mid_atl_sentiment_nrc <- mid_atl %>% inner_join(get_sentiments('nrc'))
mid_atl_sentiment_bing <- mid_atl %>% inner_join(get_sentiments('bing'))

```

#### Plot Sentiment 
```{r}
ggplot(data = mid_atl_sentiment_affin, aes(x=value))+geom_histogram()+ggtitle("Mid Atlantic Region Sentiment Range")+theme_minimal()
```
#### Another way to analyze Sentiment 
Here we use the ggplot package 
```{r}
set.seed(42)
ggplot(mid_atl[1:100,], aes(label = word, size = n))+geom_text_wordcloud()+theme_minimal()

```
## NorthEast Region 
#### Load in NorthEast Region File
```{r} 
# import data from compiled newspaper reading
north <- read_lines("NorthEastRegion.txt")
# make mid_atl into a tibble data structure
north <- tibble(north)
```

#### Word Frequencies
1. The first step in the process is finding the frequency of words per region. 
```{r}
#change the data frame variables in mid_atl$mid_atl character types
north$north <- as.character(north$north)

north <- north %>% 
  unnest_tokens(word, north) %>%
  anti_join(stop_words) %>% count(word, sort=TRUE)
```

#### Sentiment Analysis 
```{r}
north_sentiment_affin <- north %>% inner_join(get_sentiments('afinn'))
north_sentiment_nrc <- north %>% inner_join(get_sentiments('nrc'))
north_sentiment_bing <- north %>% inner_join(get_sentiments('bing'))

```

#### Plot Sentiment 
```{r}
ggplot(data = north_sentiment_affin, aes(x=value))+geom_histogram()+ggtitle("Northeast Region Sentiment Range")+theme_minimal()
```
#### Another way to analyze Sentiment 
Here we use the ggplot package 
```{r}
set.seed(42)
ggplot(north[1:100,], aes(label = word, size = n))+geom_text_wordcloud()+theme_minimal()

```



```{r}


```
Please submit a cleanly knitted HTML file describing in detail the steps you 
took along the way, the results of your analysis and most importantly the implications/next steps you would recommend.  You will report your final results and recommendations next week in class. This will be 5 minutes per group. 

